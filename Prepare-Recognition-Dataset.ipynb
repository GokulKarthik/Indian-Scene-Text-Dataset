{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Hindi\n",
    "\n",
    "Dataset Source: https://drive.google.com/open?id=1E5kI8CLoC-XffqQMTWwSpBIPp1Wb2tne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Merge Annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_dir = \"Data/Synthetic-Hindi/Annotation/{}\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file_path = \"Data/Annotations/Annotations-Hindi.csv\"\n",
    "cols = ['x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4', 'text', 'folder_id', 'file_id']\n",
    "ann_df = pd.DataFrame(columns=cols)\n",
    "ann_df.to_csv(file_path, index=False)\n",
    "for i in tqdm(range(1, 25+1)):\n",
    "    file_names = os.listdir(ann_dir.format(i))\n",
    "    for file_name in tqdm(file_names, leave=False):\n",
    "        local_file_path = os.path.join(ann_dir.format(i), file_name)\n",
    "        df = pd.read_csv(local_file_path, sep=' ', header=None)\n",
    "        df.columns = ['x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4', 'text']\n",
    "        df['folder_id'] = i\n",
    "        df['file_id'] = file_name.split(\".\")[0]\n",
    "        #df = df[cols]\n",
    "        df.to_csv(file_path, index=False, header=None, mode='a')\n",
    "        \n",
    "ann_df = pd.read_csv(file_path)\n",
    "print(ann_df.shape)\n",
    "ann_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df = pd.DataFrame()\n",
    "for i in tqdm(range(1, 25+1)):\n",
    "    file_names = os.listdir(ann_dir.format(i))\n",
    "    for file_name in tqdm(file_names, leave=False):\n",
    "        local_file_path = os.path.join(ann_dir.format(i), file_name)\n",
    "        df = pd.read_csv(local_file_path, sep=' ', header=None)\n",
    "        df.columns = ['x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4', 'text']\n",
    "        df['folder_id'] = i\n",
    "        df['file_id'] = file_name.split(\".\")[0]\n",
    "        ann_df = ann_df.append(df, ignore_index=True)\n",
    "print(ann_df.shape)\n",
    "ann_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['folder_id', 'file_id', 'text','x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']\n",
    "ann_df = ann_df[cols]\n",
    "ann_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Filter improper data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df = pd.read_csv(\"Data/Annotations/Annotations-Hindi.csv\")\n",
    "characters_df = pd.read_csv(\"Data/Characters/Characters-Hindi.csv\")\n",
    "glyphs = list(characters_df['Glyph'].unique())\n",
    "consonants = list(characters_df['Consonant'].unique())\n",
    "vowels = characters_df[characters_df['Consonant']==\"-\"]['Character'].values.tolist()\n",
    "characters = glyphs + consonants + vowels\n",
    "print(len(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(text):\n",
    "\n",
    "    corrected_text = \"\".join([character for character in text if character in characters])\n",
    "    \n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df['corrected_text'] = ann_df['text'].progress_apply(correct_text)\n",
    "ann_df['num_removals'] = ann_df['text'].str.len() - ann_df['corrected_text'].str.len()\n",
    "ann_df['image_id'] = ann_df['folder_id'].astype('str') + \"/\" + ann_df['file_id'].astype('str') + \".jpg\"\n",
    "coords_cols = ['x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4']\n",
    "ann_df[coords_cols] = ann_df[coords_cols].clip(lower=0)\n",
    "ann_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((ann_df['num_removals']>0).sum(), len(ann_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ann_df))\n",
    "ann_df = ann_df[ann_df['num_removals']==0]\n",
    "ann_df = ann_df.drop(columns=['corrected_text', 'num_removals'])\n",
    "print(len(ann_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df.groupby('folder_id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ann_df[ann_df['folder_id'] <= 23]\n",
    "print(train.shape)\n",
    "train = train.sample(n=500000, random_state=0)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = ann_df[ann_df['folder_id'] == 24]\n",
    "print(val.shape)\n",
    "val = val.sample(n=5000, random_state=0)\n",
    "print(val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ann_df[ann_df['folder_id'] == 25]\n",
    "print(test.shape)\n",
    "test = test.sample(n=5000, random_state=0)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"Data/Hindi/Train\"\n",
    "if os.path.exists(train_dir):\n",
    "    shutil.rmtree(train_dir)\n",
    "os.makedirs(train_dir)\n",
    "\n",
    "val_dir = \"Data/Hindi/Val\"\n",
    "if os.path.exists(val_dir):\n",
    "    shutil.rmtree(val_dir)\n",
    "os.makedirs(val_dir)\n",
    "\n",
    "test_dir = \"Data/Hindi/Test\"\n",
    "if os.path.exists(test_dir):\n",
    "    shutil.rmtree(test_dir)\n",
    "os.makedirs(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_crops(data, data_dir, method=1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    method: int\n",
    "        1 - bounding rect\n",
    "        2 - min area rect\n",
    "    \"\"\"\n",
    "    coords_cols = ['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']\n",
    "    crop_size = (200, 50)\n",
    "    for image_id, image_data in tqdm(data.groupby('image_id')):\n",
    "        image_path = \"Data/Synthetic-Hindi/Image/\" + image_id\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        i = 0\n",
    "        #for row_id, row in tqdm(image_data.iterrows(), leave=False, total=len(image_data)):\n",
    "        for row_id, row in image_data.iterrows():\n",
    "            coords = row[coords_cols].values\n",
    "            coords = coords.reshape(4, 1, 2).astype(np.float32)\n",
    "            \n",
    "            if method==1:\n",
    "                x, y, w, h = cv2.boundingRect(coords)\n",
    "                crop = image[y:y+h, x:x+w]\n",
    "            elif method==2:\n",
    "                raise NotImplementedError()\n",
    "            \n",
    "            crop = cv2.resize(crop, crop_size)\n",
    "            crop_path = f\"{data_dir}/{row['text']}_{row['folder_id']}_{row['file_id']}_{i}.jpg\"\n",
    "            cv2.imwrite(crop_path, crop)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_crops(train, train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_crops(val, val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_crops(test, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Tamil\n",
    "\n",
    "Dataset Source: https://drive.google.com/drive/u/0/folders/1hnNxuHbBBZrrI7Ee6FePTsUfW97qrJAS\n",
    "(Tamil 1-30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Merge Annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_dir = \"Data/Synthetic-Tamil/Annotation/{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Data/Annotations/Annotations-Tamil.csv\"\n",
    "cols = ['x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4', 'text', 'folder_id', 'file_id']\n",
    "ann_df = pd.DataFrame(columns=cols)\n",
    "ann_df.to_csv(file_path, index=False)\n",
    "for i in tqdm(range(1, 30+1)):\n",
    "    file_names = os.listdir(ann_dir.format(i))\n",
    "    for file_name in tqdm(file_names, leave=False):\n",
    "        local_file_path = os.path.join(ann_dir.format(i), file_name)\n",
    "        df = pd.read_csv(local_file_path, sep=' ', header=None)\n",
    "        df.columns = ['x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4', 'text']\n",
    "        df['folder_id'] = i\n",
    "        df['file_id'] = file_name.split(\".\")[0]\n",
    "        df.to_csv(file_path, index=False, header=None, mode='a')\n",
    "        \n",
    "ann_df = pd.read_csv(file_path)\n",
    "print(ann_df.shape)\n",
    "ann_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['folder_id', 'file_id', 'text','x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']\n",
    "ann_df = ann_df[cols]\n",
    "ann_df.to_csv(\"Data/Annotations-Tamil.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Filter improper data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df = pd.read_csv(\"Data/Annotations-Tamil.csv\")\n",
    "characters_df = pd.read_csv(\"Data/Characters/Characters-Tamil.csv\")\n",
    "glyphs = list(characters_df['Glyph'].unique())\n",
    "consonants = list(characters_df['Consonant'].unique())\n",
    "vowels = characters_df[characters_df['Consonant']==\"-\"]['Character'].values.tolist()\n",
    "characters = glyphs + consonants + vowels\n",
    "print(len(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(text):\n",
    "\n",
    "    corrected_text = \"\".join([character for character in text if character in characters])\n",
    "    \n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df['corrected_text'] = ann_df['text'].progress_apply(correct_text)\n",
    "ann_df['num_removals'] = ann_df['text'].str.len() - ann_df['corrected_text'].str.len()\n",
    "ann_df['image_id'] = ann_df['folder_id'].astype('str') + \"/\" + ann_df['file_id'].astype('str') + \".jpg\"\n",
    "coords_cols = ['x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4']\n",
    "ann_df[coords_cols] = ann_df[coords_cols].clip(lower=0)\n",
    "ann_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((ann_df['num_removals']>0).sum(), len(ann_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ann_df))\n",
    "ann_df = ann_df[ann_df['num_removals']==0]\n",
    "ann_df = ann_df.drop(columns=['corrected_text', 'num_removals'])\n",
    "print(len(ann_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df.groupby('folder_id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ann_df[ann_df['folder_id'] <= 28]\n",
    "print(train.shape)\n",
    "train = train.sample(n=500000, random_state=0)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = ann_df[ann_df['folder_id'] == 29]\n",
    "print(val.shape)\n",
    "val = val.sample(n=5000, random_state=0)\n",
    "print(val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ann_df[ann_df['folder_id'] == 30]\n",
    "print(test.shape)\n",
    "test = test.sample(n=5000, random_state=0)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"Data/Tamil/Train\"\n",
    "if os.path.exists(train_dir):\n",
    "    shutil.rmtree(train_dir)\n",
    "os.makedirs(train_dir)\n",
    "\n",
    "val_dir = \"Data/Tamil/Val\"\n",
    "if os.path.exists(val_dir):\n",
    "    shutil.rmtree(val_dir)\n",
    "os.makedirs(val_dir)\n",
    "\n",
    "test_dir = \"Data/Tamil/Test\"\n",
    "if os.path.exists(test_dir):\n",
    "    shutil.rmtree(test_dir)\n",
    "os.makedirs(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_crops(data, data_dir, method=1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    method: int\n",
    "        1 - bounding rect\n",
    "        2 - min area rect\n",
    "    \"\"\"\n",
    "    coords_cols = ['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']\n",
    "    crop_size = (200, 50)\n",
    "    for image_id, image_data in tqdm(data.groupby('image_id')):\n",
    "        image_path = \"Data/Synthetic-Tamil/Image/\" + image_id\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        i = 0\n",
    "        #for row_id, row in tqdm(image_data.iterrows(), leave=False, total=len(image_data)):\n",
    "        for row_id, row in image_data.iterrows():\n",
    "            coords = row[coords_cols].values\n",
    "            coords = coords.reshape(4, 1, 2).astype(np.float32)\n",
    "            \n",
    "            if method==1:\n",
    "                x, y, w, h = cv2.boundingRect(coords)\n",
    "                crop = image[y:y+h, x:x+w]\n",
    "            elif method==2:\n",
    "                raise NotImplementedError()\n",
    "            \n",
    "            crop = cv2.resize(crop, crop_size)\n",
    "            crop_path = f\"{data_dir}/{row['text']}_{row['folder_id']}_{row['file_id']}_{i}.jpg\"\n",
    "            cv2.imwrite(crop_path, crop)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_crops(train, train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_crops(val, val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_crops(test, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Malayalam\n",
    "\n",
    "Dataset Source: https://drive.google.com/drive/u/0/folders/1hnNxuHbBBZrrI7Ee6FePTsUfW97qrJAS\n",
    "(Malayalam 1-30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Merge Annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_dir = \"Data/Synthetic-Malayalam/Annotation/{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Data/Annotations/Annotations-Malayalam.csv\"\n",
    "cols = ['x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4', 'text', 'folder_id', 'file_id']\n",
    "ann_df = pd.DataFrame(columns=cols)\n",
    "ann_df.to_csv(file_path, index=False)\n",
    "for i in tqdm(range(1, 30+1)):\n",
    "    file_names = os.listdir(ann_dir.format(i))\n",
    "    for file_name in tqdm(file_names, leave=False):\n",
    "        local_file_path = os.path.join(ann_dir.format(i), file_name)\n",
    "        df = pd.read_csv(local_file_path, sep=' ', header=None)\n",
    "        df.columns = ['x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4', 'text']\n",
    "        df['folder_id'] = i\n",
    "        df['file_id'] = file_name.split(\".\")[0]\n",
    "        df.to_csv(file_path, index=False, header=None, mode='a')\n",
    "        \n",
    "ann_df = pd.read_csv(file_path)\n",
    "print(ann_df.shape)\n",
    "ann_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['folder_id', 'file_id', 'text','x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']\n",
    "ann_df = ann_df[cols]\n",
    "ann_df.to_csv(\"Data/Annotations-Malayalam.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 Filter improper data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df = pd.read_csv(\"Data/Annotations/Annotations-Malayalam.csv\")\n",
    "characters_df = pd.read_csv(\"Data/Characters/Characters-Malayalam.csv\")\n",
    "glyphs = list(characters_df['Glyph'].unique())\n",
    "consonants = list(characters_df['Consonant'].unique())\n",
    "vowels = characters_df[characters_df['Consonant']==\"-\"]['Character'].values.tolist()\n",
    "characters = glyphs + consonants + vowels\n",
    "print(len(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(text):\n",
    "\n",
    "    corrected_text = \"\".join([character for character in text if character in characters])\n",
    "    \n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df['corrected_text'] = ann_df['text'].progress_apply(correct_text)\n",
    "ann_df['num_removals'] = ann_df['text'].str.len() - ann_df['corrected_text'].str.len()\n",
    "ann_df['image_id'] = ann_df['folder_id'].astype('str') + \"/\" + ann_df['file_id'].astype('str') + \".jpg\"\n",
    "coords_cols = ['x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4']\n",
    "ann_df[coords_cols] = ann_df[coords_cols].clip(lower=0)\n",
    "ann_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((ann_df['num_removals']>0).sum(), len(ann_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df[ann_df['num_removals']>0][['text', 'corrected_text', 'image_id']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ann_df))\n",
    "ann_df = ann_df[ann_df['num_removals']==0]\n",
    "ann_df = ann_df.drop(columns=['corrected_text', 'num_removals'])\n",
    "print(len(ann_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.3 Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df.groupby('folder_id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ann_df[ann_df['folder_id'] <= 20]\n",
    "print(train.shape)\n",
    "train = train.sample(n=400000, random_state=0)\n",
    "print(train.shape)\n",
    "\n",
    "val = ann_df[ann_df['folder_id'] == 21]\n",
    "print(val.shape)\n",
    "val = val.sample(n=5000, random_state=0)\n",
    "print(val.shape)\n",
    "\n",
    "test = ann_df[ann_df['folder_id'] == 22]\n",
    "print(test.shape)\n",
    "test = test.sample(n=5000, random_state=0)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"Data/Malayalam/Train\"\n",
    "if os.path.exists(train_dir):\n",
    "    shutil.rmtree(train_dir)\n",
    "os.makedirs(train_dir)\n",
    "\n",
    "val_dir = \"Data/Malayalam/Val\"\n",
    "if os.path.exists(val_dir):\n",
    "    shutil.rmtree(val_dir)\n",
    "os.makedirs(val_dir)\n",
    "\n",
    "test_dir = \"Data/Malayalam/Test\"\n",
    "if os.path.exists(test_dir):\n",
    "    shutil.rmtree(test_dir)\n",
    "os.makedirs(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_crops(data, data_dir, method=1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    method: int\n",
    "        1 - bounding rect\n",
    "        2 - min area rect\n",
    "    \"\"\"\n",
    "    coords_cols = ['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']\n",
    "    crop_size = (200, 50)\n",
    "    for image_id, image_data in tqdm(data.groupby('image_id')):\n",
    "        image_path = \"Data/Synthetic-Malayalam/Image/\" + image_id\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        i = 0\n",
    "        #for row_id, row in tqdm(image_data.iterrows(), leave=False, total=len(image_data)):\n",
    "        for row_id, row in image_data.iterrows():\n",
    "            coords = row[coords_cols].values\n",
    "            coords = coords.reshape(4, 1, 2).astype(np.float32)\n",
    "            \n",
    "            if method==1:\n",
    "                x, y, w, h = cv2.boundingRect(coords)\n",
    "                crop = image[y:y+h, x:x+w]\n",
    "            elif method==2:\n",
    "                raise NotImplementedError()\n",
    "            \n",
    "            crop = cv2.resize(crop, crop_size)\n",
    "            crop_path = f\"{data_dir}/{row['text']}_{row['folder_id']}_{row['file_id']}_{i}.jpg\"\n",
    "            cv2.imwrite(crop_path, crop)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_crops(train, train_dir)\n",
    "save_crops(val, val_dir)\n",
    "save_crops(test, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Telugu\n",
    "\n",
    "Dataset Source: https://drive.google.com/drive/u/0/folders/1hnNxuHbBBZrrI7Ee6FePTsUfW97qrJAS\n",
    "(Telugu 1-30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.1 Merge Annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_dir = \"Data/Synthetic-Telugu/Annotation/{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Data/Annotations/Annotations-Telugu.csv\"\n",
    "cols = ['x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4', 'text', 'folder_id', 'file_id']\n",
    "ann_df = pd.DataFrame(columns=cols)\n",
    "ann_df.to_csv(file_path, index=False)\n",
    "for i in tqdm(range(1, 30+1)):\n",
    "    file_names = os.listdir(ann_dir.format(i))\n",
    "    for file_name in tqdm(file_names, leave=False):\n",
    "        local_file_path = os.path.join(ann_dir.format(i), file_name)\n",
    "        df = pd.read_csv(local_file_path, sep=' ', header=None)\n",
    "        df.columns = ['x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4', 'text']\n",
    "        df['folder_id'] = i\n",
    "        df['file_id'] = file_name.split(\".\")[0]\n",
    "        df.to_csv(file_path, index=False, header=None, mode='a')\n",
    "        \n",
    "ann_df = pd.read_csv(file_path)\n",
    "print(ann_df.shape)\n",
    "ann_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['folder_id', 'file_id', 'text','x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']\n",
    "ann_df = ann_df[cols]\n",
    "ann_df.to_csv(\"Data/Annotations-Telugu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.2 Filter improper data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df = pd.read_csv(\"Data/Annotations/Annotations-Telugu.csv\")\n",
    "characters_df = pd.read_csv(\"Data/Characters/Characters-Telugu.csv\")\n",
    "glyphs = list(characters_df['Glyph'].unique())\n",
    "consonants = list(characters_df['Consonant'].unique())\n",
    "vowels = characters_df[characters_df['Consonant']==\"-\"]['Character'].values.tolist()\n",
    "characters = glyphs + consonants + vowels\n",
    "print(len(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(text):\n",
    "\n",
    "    corrected_text = \"\".join([character for character in text if character in characters])\n",
    "    \n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df['corrected_text'] = ann_df['text'].progress_apply(correct_text)\n",
    "ann_df['num_removals'] = ann_df['text'].str.len() - ann_df['corrected_text'].str.len()\n",
    "ann_df['image_id'] = ann_df['folder_id'].astype('str') + \"/\" + ann_df['file_id'].astype('str') + \".jpg\"\n",
    "coords_cols = ['x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4']\n",
    "ann_df[coords_cols] = ann_df[coords_cols].clip(lower=0)\n",
    "ann_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((ann_df['num_removals']>0).sum(), len(ann_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ann_df))\n",
    "ann_df = ann_df[ann_df['num_removals']==0]\n",
    "ann_df = ann_df.drop(columns=['corrected_text', 'num_removals'])\n",
    "print(len(ann_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.3 Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df.groupby('folder_id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ann_df[ann_df['folder_id'] <= 28]\n",
    "print(train.shape)\n",
    "train = train.sample(n=500000, random_state=0)\n",
    "print(train.shape)\n",
    "\n",
    "val = ann_df[ann_df['folder_id'] == 29]\n",
    "print(val.shape)\n",
    "val = val.sample(n=5000, random_state=0)\n",
    "print(val.shape)\n",
    "\n",
    "test = ann_df[ann_df['folder_id'] == 30]\n",
    "print(test.shape)\n",
    "test = test.sample(n=5000, random_state=0)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"Data/Telugu/Train\"\n",
    "if os.path.exists(train_dir):\n",
    "    shutil.rmtree(train_dir)\n",
    "os.makedirs(train_dir)\n",
    "\n",
    "val_dir = \"Data/Telugu/Val\"\n",
    "if os.path.exists(val_dir):\n",
    "    shutil.rmtree(val_dir)\n",
    "os.makedirs(val_dir)\n",
    "\n",
    "test_dir = \"Data/Telugu/Test\"\n",
    "if os.path.exists(test_dir):\n",
    "    shutil.rmtree(test_dir)\n",
    "os.makedirs(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_crops(data, data_dir, method=1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    method: int\n",
    "        1 - bounding rect\n",
    "        2 - min area rect\n",
    "    \"\"\"\n",
    "    coords_cols = ['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']\n",
    "    crop_size = (200, 50)\n",
    "    for image_id, image_data in tqdm(data.groupby('image_id')):\n",
    "        image_path = \"Data/Synthetic-Telugu/Image/\" + image_id\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        i = 0\n",
    "        #for row_id, row in tqdm(image_data.iterrows(), leave=False, total=len(image_data)):\n",
    "        for row_id, row in image_data.iterrows():\n",
    "            coords = row[coords_cols].values\n",
    "            coords = coords.reshape(4, 1, 2).astype(np.float32)\n",
    "            \n",
    "            if method==1:\n",
    "                x, y, w, h = cv2.boundingRect(coords)\n",
    "                crop = image[y:y+h, x:x+w]\n",
    "            elif method==2:\n",
    "                raise NotImplementedError()\n",
    "            \n",
    "            crop = cv2.resize(crop, crop_size)\n",
    "            crop_path = f\"{data_dir}/{row['text']}_{row['folder_id']}_{row['file_id']}_{i}.jpg\"\n",
    "            cv2.imwrite(crop_path, crop)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_crops(train, train_dir)\n",
    "save_crops(val, val_dir)\n",
    "save_crops(test, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Punjabi\n",
    "\n",
    "Dataset Source: https://drive.google.com/drive/u/0/folders/1hnNxuHbBBZrrI7Ee6FePTsUfW97qrJAS\n",
    "(Punjabi 1-30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.1 Merge Annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_dir = \"Data/Synthetic-Punjabi/Annotation/{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Data/Annotations/Annotations-Punjabi.csv\"\n",
    "cols = ['x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4', 'text', 'folder_id', 'file_id']\n",
    "ann_df = pd.DataFrame(columns=cols)\n",
    "ann_df.to_csv(file_path, index=False)\n",
    "for i in tqdm(range(1, 30+1)):\n",
    "    file_names = os.listdir(ann_dir.format(i))\n",
    "    for file_name in tqdm(file_names, leave=False):\n",
    "        local_file_path = os.path.join(ann_dir.format(i), file_name)\n",
    "        df = pd.read_csv(local_file_path, sep=' ', header=None)\n",
    "        df.columns = ['x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4', 'text']\n",
    "        df['folder_id'] = i\n",
    "        df['file_id'] = file_name.split(\".\")[0]\n",
    "        df.to_csv(file_path, index=False, header=None, mode='a')\n",
    "        \n",
    "ann_df = pd.read_csv(file_path)\n",
    "print(ann_df.shape)\n",
    "ann_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['folder_id', 'file_id', 'text','x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']\n",
    "ann_df = ann_df[cols]\n",
    "ann_df.to_csv(\"Data/Annotations-Punjabi.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.2 Filter improper data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df = pd.read_csv(\"Data/Annotations/Annotations-Punjabi.csv\")\n",
    "characters_df = pd.read_csv(\"Data/Characters/Characters-Punjabi.csv\")\n",
    "glyphs = list(characters_df['Glyph'].unique())\n",
    "consonants = list(characters_df['Consonant'].unique())\n",
    "vowels = characters_df[characters_df['Consonant']==\"-\"]['Character'].values.tolist()\n",
    "characters = glyphs + consonants + vowels\n",
    "print(len(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(text):\n",
    "\n",
    "    corrected_text = \"\".join([character for character in text if character in characters])\n",
    "    \n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df['corrected_text'] = ann_df['text'].progress_apply(correct_text)\n",
    "ann_df['num_removals'] = ann_df['text'].str.len() - ann_df['corrected_text'].str.len()\n",
    "ann_df['image_id'] = ann_df['folder_id'].astype('str') + \"/\" + ann_df['file_id'].astype('str') + \".jpg\"\n",
    "coords_cols = ['x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4']\n",
    "ann_df[coords_cols] = ann_df[coords_cols].clip(lower=0)\n",
    "ann_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((ann_df['num_removals']>0).sum(), len(ann_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ann_df))\n",
    "ann_df = ann_df[ann_df['num_removals']==0]\n",
    "ann_df = ann_df.drop(columns=['corrected_text', 'num_removals'])\n",
    "print(len(ann_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.3 Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df.groupby('folder_id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ann_df[ann_df['folder_id'] <= 28]\n",
    "print(train.shape)\n",
    "train = train.sample(n=500000, random_state=0)\n",
    "print(train.shape)\n",
    "\n",
    "val = ann_df[ann_df['folder_id'] == 29]\n",
    "print(val.shape)\n",
    "val = val.sample(n=5000, random_state=0)\n",
    "print(val.shape)\n",
    "\n",
    "test = ann_df[ann_df['folder_id'] == 30]\n",
    "print(test.shape)\n",
    "test = test.sample(n=5000, random_state=0)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"Data/Punjabi/Train\"\n",
    "if os.path.exists(train_dir):\n",
    "    shutil.rmtree(train_dir)\n",
    "os.makedirs(train_dir)\n",
    "\n",
    "val_dir = \"Data/Punjabi/Val\"\n",
    "if os.path.exists(val_dir):\n",
    "    shutil.rmtree(val_dir)\n",
    "os.makedirs(val_dir)\n",
    "\n",
    "test_dir = \"Data/Punjabi/Test\"\n",
    "if os.path.exists(test_dir):\n",
    "    shutil.rmtree(test_dir)\n",
    "os.makedirs(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_crops(data, data_dir, method=1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    method: int\n",
    "        1 - bounding rect\n",
    "        2 - min area rect\n",
    "    \"\"\"\n",
    "    coords_cols = ['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']\n",
    "    crop_size = (200, 50)\n",
    "    for image_id, image_data in tqdm(data.groupby('image_id')):\n",
    "        image_path = \"Data/Synthetic-Punjabi/Image/\" + image_id\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        i = 0\n",
    "        #for row_id, row in tqdm(image_data.iterrows(), leave=False, total=len(image_data)):\n",
    "        for row_id, row in image_data.iterrows():\n",
    "            coords = row[coords_cols].values\n",
    "            coords = coords.reshape(4, 1, 2).astype(np.float32)\n",
    "            \n",
    "            if method==1:\n",
    "                x, y, w, h = cv2.boundingRect(coords)\n",
    "                crop = image[y:y+h, x:x+w]\n",
    "            elif method==2:\n",
    "                raise NotImplementedError()\n",
    "            \n",
    "            crop = cv2.resize(crop, crop_size)\n",
    "            crop_path = f\"{data_dir}/{row['text']}_{row['folder_id']}_{row['file_id']}_{i}.jpg\"\n",
    "            cv2.imwrite(crop_path, crop)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_crops(train, train_dir)\n",
    "save_crops(val, val_dir)\n",
    "save_crops(test, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Error: \n",
    "10/915.jpg 213646\n",
    "14/2061.jpg 279943\n",
    "21/309.jpg 422288\n",
    "21/309.jpg 422290\n",
    "21/309.jpg 422289\n",
    "22/3198.jpg 440716\n",
    "22/3198.jpg 440715\n",
    "22/3198.jpg 440714\n",
    "24/2489.jpg 478702\n",
    "8/2646.jpg 156305\n",
    "8/2646.jpg 156304\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
